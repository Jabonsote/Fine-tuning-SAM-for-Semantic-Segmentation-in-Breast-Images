{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5517b9bb-f893-4efa-8d2d-1d4488dbe83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 23:00:53.991318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 23:00:54.990094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "03/05/2024 23:00:56 - INFO - torch.distributed.nn.jit.instantiator -   Created a temporary directory at /tmp/tmpbobw1na8\n",
      "03/05/2024 23:00:56 - INFO - torch.distributed.nn.jit.instantiator -   Writing /tmp/tmpbobw1na8/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "from collections import defaultdict, deque\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from sahi.utils.coco import Coco\n",
    "from sahi.utils.cv import get_bool_mask_from_coco_segmentation\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.distributed as dist\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from transformers.models.maskformer.modeling_maskformer import dice_loss, sigmoid_focal_loss\n",
    "\n",
    "# Add the SAM directory to the system path\n",
    "sys.path.append(\"./segment-anything\")\n",
    "from segment_anything import sam_model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1327210b-a429-4b90-9320-e12b5d756961",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 0  # https://github.com/pytorch/pytorch/issues/42518\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "\n",
    "# Source: https://github.com/facebookresearch/detectron2/blob/main/detectron2/utils/comm.py\n",
    "def get_world_size():\n",
    "    if not dist.is_available():\n",
    "        return 1\n",
    "    if not dist.is_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "# Source: https://github.com/facebookresearch/detectron2/blob/main/detectron2/utils/comm.py\n",
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    # serialized to a Tensor\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
    "\n",
    "    # obtain Tensor size of each rank\n",
    "    local_size = torch.LongTensor([tensor.numel()]).to(\"cuda\")\n",
    "    size_list = [torch.LongTensor([0]).to(\"cuda\") for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    # receiving Tensor from all ranks\n",
    "    # we pad the tensor because torch all_gather does not support\n",
    "    # gathering tensors of different shapes\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(\"cuda\"))\n",
    "    if local_size != max_size:\n",
    "        padding = torch.ByteTensor(size=(max_size - local_size,)).to(\"cuda\")\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddee206a-a317-4151-a47e-a51cd6dae11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco mask style dataloader\n",
    "class Coco2MaskDataset(Dataset):\n",
    "    def __init__(self, data_root, split, image_size):\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "        annotation = os.path.join(data_root, split, \"_annotations.coco.json\")\n",
    "        self.coco = Coco.from_coco_dict_or_path(annotation)\n",
    "\n",
    "        # TODO: use ResizeLongestSide and pad to square\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.image_resize = transforms.Resize((image_size, image_size), interpolation=Image.BILINEAR)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco_image = self.coco.images[index]\n",
    "        image = Image.open(os.path.join(self.data_root, self.split, coco_image.file_name)).convert(\"RGB\")\n",
    "        original_width, original_height = image.width, image.height\n",
    "        ratio_h = self.image_size / image.height\n",
    "        ratio_w = self.image_size / image.width\n",
    "        image = self.image_resize(image)\n",
    "        image = self.to_tensor(image)\n",
    "        image = self.normalize(image)\n",
    "\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "        labels = []\n",
    "        for annotation in coco_image.annotations:\n",
    "            x, y, w, h = annotation.bbox\n",
    "            # get scaled bbox in xyxy format\n",
    "            bbox = [x * ratio_w, y * ratio_h, (x + w) * ratio_w, (y + h) * ratio_h]\n",
    "            mask = get_bool_mask_from_coco_segmentation(annotation.segmentation, original_width, original_height)\n",
    "            mask = cv2.resize(mask, (self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "            mask = (mask > 0.5).astype(np.uint8)\n",
    "            label = annotation.category_id\n",
    "            bboxes.append(bbox)\n",
    "            masks.append(mask)\n",
    "            labels.append(label)\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        labels = np.stack(labels, axis=0)\n",
    "        return image, torch.tensor(bboxes), torch.tensor(masks).long()\n",
    "    \n",
    "    @classmethod\n",
    "    def collate_fn(cls, batch):\n",
    "        images, bboxes, masks = zip(*batch)\n",
    "        images = torch.stack(images, dim=0)\n",
    "        return images, bboxes, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78fc08a-2726-4373-a452-8c6e51c3bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMFinetuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_type,\n",
    "            checkpoint_path,\n",
    "            freeze_image_encoder=False,\n",
    "            freeze_prompt_encoder=False,\n",
    "            freeze_mask_decoder=False,\n",
    "            batch_size=1,\n",
    "            learning_rate=1e-4,\n",
    "            weight_decay=1e-4,\n",
    "            train_dataset=None,\n",
    "            val_dataset=None,\n",
    "            metrics_interval=10,\n",
    "        ):\n",
    "        super(SAMFinetuner, self).__init__()\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.model = sam_model_registry[self.model_type](checkpoint=checkpoint_path)\n",
    "        self.model.to(device=self.device)\n",
    "        self.freeze_image_encoder = freeze_image_encoder\n",
    "        if freeze_image_encoder:\n",
    "            for param in self.model.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if freeze_prompt_encoder:\n",
    "            for param in self.model.prompt_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if freeze_mask_decoder:\n",
    "            for param in self.model.mask_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "        self.train_metric = defaultdict(lambda: deque(maxlen=metrics_interval))\n",
    "\n",
    "        self.metrics_interval = metrics_interval\n",
    "\n",
    "    \n",
    "    def lr_scheduler_step(self, current_epoch, current_step=None, *args, **kwargs):\n",
    "        warmup_steps = 250\n",
    "        milestones = [0.66667, 0.86666]\n",
    "        gamma = 0.1\n",
    "    \n",
    "        if current_step is not None:\n",
    "            steps = current_step\n",
    "        else:\n",
    "            steps = current_epoch * self.trainer.estimated_steps_per_epoch\n",
    "    \n",
    "        if steps < warmup_steps:\n",
    "            lr_scale = (steps + 1.) / float(warmup_steps)\n",
    "        else:\n",
    "            lr_scale = 1.\n",
    "            for milestone in sorted(milestones):\n",
    "                if steps >= milestone * self.trainer.estimated_steps_per_epoch:\n",
    "                    lr_scale *= gamma\n",
    "    \n",
    "        return lr_scale\n",
    "\n",
    "\n",
    "    def forward(self, imgs, bboxes, labels):\n",
    "        _, _, H, W = imgs.shape\n",
    "        features = self.model.image_encoder(imgs)\n",
    "        num_masks = sum([len(b) for b in bboxes])\n",
    "\n",
    "        loss_focal = loss_dice = loss_iou = 0.\n",
    "        predictions = []\n",
    "        tp, fp, fn, tn = [], [], [], []\n",
    "        for feature, bbox, label in zip(features, bboxes, labels):\n",
    "            # Embed prompts\n",
    "            sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=bbox,\n",
    "                masks=None,\n",
    "            )\n",
    "            # Predict masks\n",
    "            low_res_masks, iou_predictions = self.model.mask_decoder(\n",
    "                image_embeddings=feature.unsqueeze(0),\n",
    "                image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "            # Upscale the masks to the original image resolution\n",
    "            masks = F.interpolate(\n",
    "                low_res_masks,\n",
    "                (H, W),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            predictions.append(masks)\n",
    "            # Compute the iou between the predicted masks and the ground truth masks\n",
    "            batch_tp, batch_fp, batch_fn, batch_tn = smp.metrics.get_stats(\n",
    "                masks,\n",
    "                label.unsqueeze(1),\n",
    "                mode='binary',\n",
    "                threshold=0.5,\n",
    "            )\n",
    "            batch_iou = smp.metrics.iou_score(batch_tp, batch_fp, batch_fn, batch_tn)\n",
    "            # Compute the loss\n",
    "            masks = masks.squeeze(1).flatten(1)\n",
    "            label = label.flatten(1)\n",
    "            loss_focal += sigmoid_focal_loss(masks, label.float(), num_masks)\n",
    "            loss_dice += dice_loss(masks, label.float(), num_masks)\n",
    "            loss_iou += F.mse_loss(iou_predictions, batch_iou, reduction='sum') / num_masks\n",
    "            tp.append(batch_tp)\n",
    "            fp.append(batch_fp)\n",
    "            fn.append(batch_fn)\n",
    "            tn.append(batch_tn)\n",
    "        return {\n",
    "            'loss': 20. * loss_focal + loss_dice + loss_iou,  # SAM default loss\n",
    "            'loss_focal': loss_focal,\n",
    "            'loss_dice': loss_dice,\n",
    "            'loss_iou': loss_iou,\n",
    "            'predictions': predictions,\n",
    "            'tp': torch.cat(tp),\n",
    "            'fp': torch.cat(fp),\n",
    "            'fn': torch.cat(fn),\n",
    "            'tn': torch.cat(tn),\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        imgs, bboxes, labels = batch\n",
    "        outputs = self(imgs, bboxes, labels)\n",
    "\n",
    "        for metric in ['tp', 'fp', 'fn', 'tn']:\n",
    "            self.train_metric[metric].append(outputs[metric])\n",
    "\n",
    "        # aggregate step metics\n",
    "        step_metrics = [torch.cat(list(self.train_metric[metric])) for metric in ['tp', 'fp', 'fn', 'tn']]\n",
    "        per_mask_iou = smp.metrics.iou_score(*step_metrics, reduction=\"micro-imagewise\")\n",
    "        metrics = {\n",
    "            \"loss\": outputs[\"loss\"],\n",
    "            \"loss_focal\": outputs[\"loss_focal\"],\n",
    "            \"loss_dice\": outputs[\"loss_dice\"],\n",
    "            \"loss_iou\": outputs[\"loss_iou\"],\n",
    "            \"train_per_mask_iou\": per_mask_iou,\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True, rank_zero_only=True)\n",
    "        return metrics\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        imgs, bboxes, labels = batch\n",
    "        outputs = self(imgs, bboxes, labels)\n",
    "        outputs.pop(\"predictions\")\n",
    "        return outputs\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if NUM_GPUS > 1:\n",
    "            outputs = all_gather(outputs)\n",
    "            # the outputs are a list of lists, so flatten it\n",
    "            outputs = [item for sublist in outputs for item in sublist]\n",
    "        # aggregate step metics\n",
    "        step_metrics = [\n",
    "            torch.cat(list([x[metric].to(self.device) for x in outputs]))\n",
    "            for metric in ['tp', 'fp', 'fn', 'tn']]\n",
    "        # per mask IoU means that we first calculate IoU score for each mask\n",
    "        # and then compute mean over these scores\n",
    "        per_mask_iou = smp.metrics.iou_score(*step_metrics, reduction=\"micro-imagewise\")\n",
    "\n",
    "        metrics = {\"val_per_mask_iou\": per_mask_iou}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        def warmup_step_lr_builder(warmup_steps, milestones, gamma):\n",
    "            def warmup_step_lr(steps):\n",
    "                if steps < warmup_steps:\n",
    "                    lr_scale = (steps + 1.) / float(warmup_steps)\n",
    "                else:\n",
    "                    lr_scale = 1.\n",
    "                    for milestone in sorted(milestones):\n",
    "                        if steps >= milestone * self.trainer.estimated_stepping_batches:\n",
    "                            lr_scale *= gamma\n",
    "                return lr_scale\n",
    "            return warmup_step_lr\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            opt,\n",
    "            warmup_step_lr_builder(250, [0.66667, 0.86666], 0.1)\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': opt,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': \"step\",\n",
    "                'frequency': 1,\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            collate_fn=self.train_dataset.collate_fn,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            shuffle=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            collate_fn=self.val_dataset.collate_fn,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            shuffle=False)\n",
    "        return val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219fc74d-a206-4308-965d-6a493f6ed2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python finetune.py data vit_h SAM/sam_vit_h_4b8939.pth m\n",
    "# NO USAR VERSION COMPRIMIDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925a331-626b-4100-8d71-e3f2a0fba311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading coco annotations: 100%|█████████████████| 9/9 [00:00<00:00, 1810.14it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading coco annotations: 100%|█████████████████| 9/9 [00:00<00:00, 8242.08it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Set your variables directly\n",
    "dataset_name = \"data\"\n",
    "model_type = \"vit_h\"\n",
    "checkpoint_path = \"SAM/sam_vit_h_4b8939.pth\"\n",
    "freeze_image_encoder = True\n",
    "freeze_prompt_encoder = False\n",
    "freeze_mask_decoder = False\n",
    "batch_size = 1\n",
    "image_size = 1024\n",
    "steps = 1500\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-2\n",
    "metrics_interval = 50\n",
    "output_dir = \"m\"\n",
    "\n",
    "# load the dataset\n",
    "train_dataset = Coco2MaskDataset(data_root=dataset_name, split=\"train\", image_size=image_size)\n",
    "val_dataset = Coco2MaskDataset(data_root=dataset_name, split=\"val\", image_size=image_size)\n",
    "\n",
    "# create the model\n",
    "model = SAMFinetuner(\n",
    "    model_type,\n",
    "    checkpoint_path,\n",
    "    freeze_image_encoder=freeze_image_encoder,\n",
    "    freeze_prompt_encoder=freeze_prompt_encoder,\n",
    "    freeze_mask_decoder=freeze_mask_decoder,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    metrics_interval=metrics_interval,\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=output_dir,\n",
    "        filename='{step}-{val_per_mask_iou:.2f}',\n",
    "        save_last=True,\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_per_mask_iou\",\n",
    "        mode=\"max\",\n",
    "        save_weights_only=True,\n",
    "        every_n_train_steps=metrics_interval,\n",
    "    ),\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    strategy='ddp' if NUM_GPUS > 1 else None,\n",
    "    accelerator=DEVICE,\n",
    "    devices=NUM_GPUS,\n",
    "    precision=32,\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=-1,\n",
    "    max_steps=steps,\n",
    "    val_check_interval=metrics_interval,\n",
    "    check_val_every_n_epoch=None,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c32d3c-5523-4bd1-9c94-0473ba4c5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56c3d8-c3b8-4f46-aaa4-a92aa13baaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf85335-a54c-4483-a4db-c0b8dba0f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "# MLFLOW metricas y prediccion\n",
    "# Segmentar imagenes (al menos 60)\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
